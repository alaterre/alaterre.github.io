<!DOCTYPE html>
<html
  dir="ltr"
  lang="en"
  data-theme="light"
><head>
  <title>
    Alexandre Laterre
      |
      Accelerated Architecture for Reinforcement Learning in Jax


    

  </title>

  
  <meta charset="utf-8" /><meta name="generator" content="Hugo 0.84.2" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta
    name="description"
    content="
      Lead Research Scientist at InstaDeep


    "
  />
  
  
  
  <link
    rel="stylesheet"
    href="/css/main.min.b65887cf357eb2ed8c77c4532bae2a6c1123ab1c6c78d98eaa3f6bea847826df.css"
    integrity="sha256-tliHzzV&#43;su2Md8RTK64qbBEjqxxseNmOqj9r6oR4Jt8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.f798cbda9aaa38f89eb38be6414bd082cfd71a6780375cbf67b6d2fb2b96491e.css"
    integrity="sha256-95jL2pqqOPies4vmQUvQgs/XGmeAN1y/Z7bS&#43;yuWSR4="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"
    integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA=="
    crossorigin="anonymous"
  />
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />

  <link rel="canonical" href="/posts/jax-rl/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.2a2cd9614b7d007dfbb75e8da19e3a0fa872ceab53c6d000c00b7a0c89b85bfc.js"
    integrity="sha256-KizZYUt9AH37t16NoZ46D6hyzqtTxtAAwAt6DIm4W/w="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.7fd87181cdd7e8413aa64b6867bb32f3a8dc242e684fc7d5bbb9f600dbc2b6eb.js"
      integrity="sha256-f9hxgc3X6EE6pktoZ7sy86jcJC5oT8fVu7n2ANvCtus="
      crossorigin="anonymous"
    ></script>

  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Accelerated Architecture for Reinforcement Learning in Jax"/>
<meta name="twitter:description" content="todo list
   Intrigue by JAX, the new cool kid on the block?"/>



  
</head>
<body>
    <header><div
  class="page-top 
    animated fadeInDown

  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true"></span>
    <span aria-hidden="true"></span>
    <span aria-hidden="true"></span>
  </a>
  <nav>
    <ul class="nav__list" id="navMenu">
      <div class="nav__links">
        
        
          
          <li>
            <a
              
              href="/"
              
              title=""
              >About</a
            >
          </li>

        
          
          <li>
            <a
              
              href="/publications"
              
              title=""
              >Publications</a
            >
          </li>

        
        
      </div>
      <li>
        
          <a class="theme-switch" title="Switch Theme">
            <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
          </a>

        
      </li>
    </ul>
  </nav>
</div>
</header>
    <div class="wrapper">
      <aside><div
  class="sidebar
    animated fadeInDown

  "
>
  <div class="sidebar__content">
    <div class="logo-title">
      <div class="title">
        <img src="/img/profile.jpg" alt="profile picture" />
        <h3 title=""><a href="/">Alex Laterre</a></h3>
        <div class="description">
          <p>Lead Research Scientist at InstaDeep</p>
        </div>
      </div>
    </div>
    <ul class="social-links">
      
        <li>
          <a href="https://twitter.com/AlexLaterre" rel="me" aria-label="Twitter">
            <i class="fab fa-twitter fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li>
          <a href="https://www.linkedin.com/in/reinforce/" rel="me" aria-label="Linked-In">
            <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li>
          <a href="https://github.com/alaterre" rel="me" aria-label="GitHub">
            <i class="fab fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li>
          <a href="https://medium.com/@alex.ltr" rel="me" aria-label="Medium">
            <i class="fab fa-medium fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li>
          <a href="https://scholar.google.com/citations?user=HrMSaicAAAAJ&amp;hl=en" rel="me" aria-label="Google Scholar">
            <i class="fab fa-google fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
        <li>
          <a href="mailto:alexandre.laterre@gmail.com" rel="me" aria-label="e-mail">
            <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>

      
    </ul>
  </div><footer class="footer footer--sidebar">
  <div class="by_farbox">
    <ul class="footer__list">
      <li class="footer__item">
        &copy;
        
          Alexandre Laterre
          2021


        
      </li>
      
    </ul>
  </div>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.71100d84fab0ad794b8399a66ac810700cc78d703f715dc10af4d7ba7b761362.js"
    integrity="sha256-cRANhPqwrXlLg5mmasgQcAzHjXA/cV3BCvTXunt2E2I="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></div>
</aside>
      <main>
        <div class="autopagerize_page_element">
          <div class="content">
  <div
    class="post 
      animated fadeInDown

    "
  >
    <div class="post-content">
      
      <div class="post-title">
        <h1>Accelerated Architecture for Reinforcement Learning in Jax</h1>
        
      </div>

      <p>todo list</p>
<ul>
<li><input disabled="" type="checkbox"> </li>
</ul>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Intrigue by <a href="https://jax.readthedocs.io/en/latest/index.html">JAX</a>, the new cool kid on the block? This blog post will bring you some clarities on the competitive advantages of using Jax for Reinforcement Learning. We will explore a specific way to architect your RL training to leverage the best of what JAX has to offer.</p>
<hr>
<h2 id="what-isjax">What is JAX?</h2>
<p>A quick high-level introduction to Jax first! Jax is a numerical computing library for machine learning in Python. Its API for writing numerical functions follows NumPy closely but augments it with a flexible system for composable program transformation, that supports automatic differentiation, SIMD-programming, just-in-time compilation, and hardware acceleration via GPUs or TPUs. In short,</p>
<h4 id="jax--numpy--autograd--xla">JAX = NumPy + AutoGrad + XLA</h4>
<p>Most of these functionalities are exposed through a one-function API. Some of them are for instance:</p>
<ul>
<li><code>jit </code>lets you just-in-time compile your own Python functions into XLA-optimized kernels and executes them on accelerators such as GPUs or TPUs. Just-in-time compilation provides substantial speed-ups and bypasses Python&rsquo;s &ldquo;global interpreter lock&rdquo; (GIL).</li>
<li><code>grad</code>: Automatic differentiation transformation, <code>grad(f)</code> returns a function corresponding to the gradient of <code>f</code>. It can differentiate through a large subset of Python&rsquo;s features, including loops, ifs, recursion, and closure.</li>
<li><code>vmap</code> is for automatic vectorization. Similar to python <code>map</code>,  It maps a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a function&rsquo;s primitive operations for better performance. As a result, one can write a function logic for a single item and automatically vectorize it to apply it to a batch.</li>
</ul>
<p>The jit-compilation, automatic differentiation, and vectorization can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without having to leave Python and your beloved Numpy-compatible API. These operations might seem abstract at the moment but will get more clear in the sequel when using them in the context of reinforcement learning.</p>
<p>To get the full picture and be completely informed, have also a look at the <a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html">sharp bits</a> of JAX as all those advantages don&rsquo;t come without a few constraints (functional programming, in-place updates, random number generation, control flow, etc). For a tutorial on JAX, I recommend its official <a href="https://jax.readthedocs.io/en/latest/index.html">documentation</a>.</p>
<hr>
<h2 id="reinforcement-learning">Reinforcement Learning </h2>
<p>Reinforcement Learning (RL) is the problem of controlling an unknown system. Through interactions with the environment, an agent learns from trial and error what works and what doesn&rsquo;t to maximize its expected cumulative reward through time. 
An RL infrastructure is a loop of data collection and training, where actors explore the environment and collect samples, which are then sent to the learners to train and update the model. Most current RL techniques require many iterations over batches of millions of samples from the environment to learn a target task. As such, an RL infrastructure should not only scale efficiently and collect an immense number of samples, but also be able to swiftly iterate over these extensive amounts of samples during training. Multiple solutions to these challenges exist:
Local inference on each actor with frequent model retrieval from the learner placed on Accelerators, e.g. Impala, Menger.</p>
<p><img src="/img/menger-rl.gif" alt="Alt text for my gif"></p>
<p>Actors interact with their environment and perform the neural network inference locally on CPUs. The learner is placed on a hardware accelerator and sends its parameters to the actors at regular time intervals. sourceCentralized inference using TPU/GPUs for performing batched calls, e.g. Google SEED-RL.</p>
<p><img src="/img/seedRL.gif" alt="Alt text for my gif"></p>
<p>Overview of the architecture of SEED RL. In contrast to the Menger architecture, the actors only take actions in environments. The inference is executed centrally by the learner on accelerators using batches of data from multiple actors. sourceDistributed Acting and Learning with synchronized gradient update, e.g. OpenAI Rapid.</p>
<p>Unfortunately, all these strategies require complex software architectures to fully leverage hardware capacities (distributed computing, CPU/GPU communication, etc) often preventing individuals or small research labs from experimenting with their research ideas.
Where does Jax come in?
Thanks to its automatic vectorization, JAX is particularly well suited to the last scaling strategy. I will describe an online learning system in which the acting and learning run both on an accelerator (GPU/TPU). This setup assumes the environment is implemented in JAX. This restricts the range of environments supported, but provides large benefits in terms of performance.
Step 1
Let&rsquo;s start by writing the training loop for a single agent and a single environment. In this setup, there is no separation between actors and learner as the same agent is responsible for collecting rollouts and updating its parameters. The training loop proceeds as follows:
Rollout Stage: The agent collect transitions by interacting with the environment using the latest available network parameters.
Update Stage: The agent then uses the collected transitions to compute the gradient of an RL loss and update its parameters.</p>
<p>Note that there is no need for the data to ever leave the accelerator nor is there any need for context switches between various processes. The whole training loop (rollouts + update) happens within a single non-interrupted (jitted) function. At the moment, we have this:
Step 2
Now comes the magic of JAX automatic vectorization! 🦄 As mentioned above, vmap can automatically vectorize Jax functions. Although the training loop feels complicated, it corresponds to a sequence of JAX operations. Hence we can apply any of the aforementioned transformations (jit, grad, vmap). To increase GPU usage, It is a common strategy to have an agent interact with a batch of environments for it to batch the inference calls for the action selection.
In Jax, this can be done using one call to vmap at the roullout stage. The data collection will automatically be vectorized without requiring any code change! 🪄
Step 3
I think you can expect what&rsquo;s coming next. Let&rsquo;s vmap again 🚀🔥. 
QUESTION: What&rsquo;s the benefit of vmap the agent rather than vmapping even more the environment.
(Optional) Step 4</p>
<p>Explain TPUs and Anakin
For more information about that setup, I suggest the reading of Anakn blabla…
I chose to use the PPO algorithm given its popularity and efficiency. However, the logic stays the same for other on-policy RL algorithms.
The details on how to average gradient accorss devices is available in the code and blabla … focus on the high-level here.
Off-Policy Learning
Note that if we were to work in an off-policy setting (e.g. SAC), one could implement a replay buffer for each worker living on its accelerator-core. The training would proceed as follows: Each available accelerator core runs the environment for a few steps and adds this data to an individual per-core replay buffer. Several SGD updates are performed, where each accelerator core samples its part of a batch from its own replay buffer, computes gradient updates, and synchronizes the final update with other cores.
Imagine bringing together the syntax and generality of NumPy with the speed and automatic differentiation capabilities of PyTorch/TensorFlow. The result is JAX.</p>
<p>Todo List
Structure
introduction to Jax (skip the long tutorial and reference nice links)
Whereas most of the Deep Learning frameworks have conditioned us to think exclusively about network inference, Jax opens up new opportunities to integrate more tightly the rest of the training workflow and system for maximum efficiency. In particular, Jax is much more than a deep learning framework. Actually, it&rsquo;s not even advertised like one. Jax is:
Numpy + AutoGrad + XLA Compilation
And actually many libraries and project appear to be using JAX outside the scope of Deep Learning (physic simulations, etc)</p>
<p>So what kind of new opportunities are we talking about. PodRacer, integrating the data collection step inside the update step of the neural network parameters. This 
What I&rsquo;m gonna explained in this work has been aprtially done in the DeepMind paper PodRacer with a focus on TPU utilization for reinfrocement in jax.
fits very well the on-policy learning setting
benchmarking of the performance - PPO on Catch and navigation task on TPU.</p>
<p>The downside, which is not minor, is that the environment must be written in Jax and therefore must respect the &lsquo;limitations&rsquo; or at least the design principles of the library, i.e. purely functional programming, … (see the sharp bits in the Jax documentation).
The PodRacer paper provides an alternative architecture, consisting of …
Jax Ecosystem: The Jax community mostly backed up by Google AI and DeepMind </p>
<p>hope it can free researhers from the cumbersome inherent to today&rsquo;s scaling infrastructure. Consequently, allowing them to shorten the development cycle and having the ability to run large scale experimentats which shouldd leadd to more robust scientifc results.</p>
<p>but not only, with great initiatives from Microsoft Research and their Jax-based RL library coax.
Acknowledgment: …
p.s. if like us, you&rsquo;re excited about the perspective Jax offers around the research in Reinforcement Learning, please do not hesitate to reach out. We are always looking for enthusiastic collaborators.
To start learning and exploring the Jax ecosystem, have a look at this GitHub repository referencing tutorials and cool projects that have been done using Jax.</p>
<hr>
<p>The code to reproduce the experiments and findings is available on my Github. Here is also a few useful references directly linked to the content of this blog:
Podracer architectures for scalable Reinforcement Learning
Talk at NeurIPS 2020: JAX ecosystem| slides
Brax: a Differentiable Physic Engine in JAX (~ MuJoCo in Jax)| Blog</p>
<p>To dive deeper into the Jax Ecosystem, have a look at this compilation of cool projects, libraries, and codes: awesome-jax.
Finally, if you share my enthusiasm for the perspectives JAX offers for the advancement of machine learning, please reach out! Happy to chat and discuss related topics 🚀s</p>
<p>The same content is available on <a href="https://medium.com/@alex.ltr">Medium</a></p>
</div>
    <div class="post-footer">
      <div class="info">
        

        
      </div>
    </div>

    
  </div>


          </div>
        </div>
      </main>
    </div><footer class="footer footer--base">
  <div class="by_farbox">
    <ul class="footer__list">
      <li class="footer__item">
        &copy;
        
          Alexandre Laterre
          2021


        
      </li>
      
    </ul>
  </div>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.71100d84fab0ad794b8399a66ac810700cc78d703f715dc10af4d7ba7b761362.js"
    integrity="sha256-cRANhPqwrXlLg5mmasgQcAzHjXA/cV3BCvTXunt2E2I="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></body>
</html>
